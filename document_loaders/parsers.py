"""Module to handle the reading of PDF files and provide it's information
as text or OCR data.
"""
from typing import Iterator
from dataclasses import dataclass

import os
import glob
import hashlib
import shutil

from pypdf import PdfReader
from pypdf._page import PageObject
from PIL import Image, ImageDraw
import pytesseract
import pdf2image
import matplotlib.pyplot as plt
import pandas as pd

from .visitors import PageTextVisitor

@dataclass
class GroupState:
    """Store state when searching groups while reconstrucing text of document."""

    line_cols: dict[str, dict[str, int]]
    group_cols: dict[str, dict[str, int]]
    prev_num_cols: int|None
    centered: dict[str, bool|None]
    prev_new_group: bool
    tolerance: float
    group_num: int

class PypdfPage():
    """This class stores the text extracted by PyPDF and provides
    methods to process it.
    """

    def __init__(self, page: PageObject):
        self.page = page

        self.visitor = PageTextVisitor()

    def get_text(self):
        """Return the text contained within the boundaries of the page."""
        self.visitor.set_boundaries(*self.page['/ArtBox'])
        text = self.page.extract_text(visitor_text=self.visitor.visitor_text)

        return self.__remove_out_of_bounds_text(text)

    def get_words(self, suffix:str = '') -> pd.DataFrame:
        """Get the text of the page and split it into words into a dataframe."""
        words = [(idx, f'{w}{suffix}') for idx, w in enumerate(self.get_text().split())]
        df_words = pd.DataFrame(words, columns=['txt_idx', 'word'])
        df_words.set_index('txt_idx', inplace=True)

        return df_words

    def __remove_out_of_bounds_text(self, text: str) -> str:
        clean_text = text
        for line in self.visitor.get_out_of_bounds_text():
            if clean_text.endswith(line):
                clean_text = clean_text[:-len(line)]
            elif clean_text.startswith(line):
                clean_text = clean_text[len(line):]

        return clean_text

class PypdfParser():
    """This is a class to parse PDF files to text using pypdf.

    :param file_path: The path of the file to be parsed.
    :type file_path: str
    """

    def __init__(self, file_path: str):
        self.file_path = file_path

        self.reader = PdfReader(self.file_path)

    def get_text(self, page_separator: str = '') -> str:
        """Return the full text of the file as extracted by pypdf.

        :param page_separator: String to be added to separate each page,
            defaults to ''.
        :type page_separator: str, optional

        :return: A string of all the text from the document
        :rtype: str
        """
        return page_separator.join([page.extract_text() for page in self.reader.pages])

    def get_num_pages(self) -> int:
        """Return the number of pages of the document."""
        return len(self.reader.pages)

    def get_pages(self) -> Iterator[PypdfPage]:
        """Read the document and return the text of each page as an Iterator.

        :return: An Iterator that yields the text of each page at a time
        :rtype: Iterator[str]
        """
        for page in self.reader.pages:
            yield PypdfPage(page)

    def get_page(self, page_num: int):
        """Return a specific page of the document."""
        return PypdfPage(self.reader.pages[page_num])

class DataReconstructor():
    """Class to fix issues generated by Tesseract.

    This class reconstruct the text of the page, creating new groups and
    adding columns to the data.
    """

    def __init__(self, data: pd.DataFrame):
        self.data = data.copy()

        self.data['right'] = self.data['left'] + self.data['width']
        self.data['bottom'] = self.data['top'] + self.data['height']

        writable_min_x, _, writable_max_x, _ = self.__get_writable_boundaries()
        self.writable_width = writable_max_x - writable_min_x
        self.writable_center = writable_min_x + self.writable_width * 0.5

    def get_reconstructed(self) -> pd.DataFrame:
        """Return the reconstructed data of the page by fixing the issues of the original data
        provided by Tesseract."""
        self.__assign_line_number()
        self.__assign_column_number()
        self.__assign_column_position()
        self.__assign_group_number()

        return self.data

    def __assign_line_number(self):
        self.data['line'] = pd.Series(dtype='int')
        words = self.data[self.data['level'] == 5].sort_values(by=['top'])

        current_line_num = -1
        current_min_y, current_max_y = 0, 0
        for word_idx, word in words.iterrows():
            word_top = word['top']
            word_bottom = word['top'] + word['height']
            word_center = word['top'] + word['height'] * 0.5

            if current_min_y < word_center < current_max_y: # Is same line
                current_min_y = min(current_min_y, word_top)
                current_max_y = max(current_max_y, word_bottom)
            else: # Is another line
                current_line_num += 1
                current_min_y, current_max_y = word_top, word_bottom

            self.data.loc[word_idx, 'line'] = current_line_num

    def __assign_column_number(self):
        self.data['column'] = pd.Series(dtype='int')
        tolerance = self.writable_width * 0.05

        for _, line_words in self.data.groupby('line'):
            col_number = -1
            current_col = {}

            sorted_words = line_words.sort_values(by='left')
            for idx, word in sorted_words.iterrows():
                word_left = word['left']
                word_right = word['left'] + word['width']

                if (current_col
                    and word_left - current_col['maxX'] < tolerance):
                    # It is close enough to be in the same column
                    current_col['maxX'] = word_right
                else:
                    col_number += 1
                    current_col = {'maxX': word_right, 'minX': word_left}

                self.data.loc[idx, 'column'] = col_number

    def __assign_column_position(self):
        self.data['col_position'] = pd.Series(dtype='int')

        grouped = self.data.groupby(['line', 'column']).agg({'left': 'min', 'right': 'max'})
        for (line, column), values in grouped.iterrows():
            position = -1
            if self.__column_is_aligned_left(values['left'], values['right']):
                position = 0
            elif self.__column_is_aligned_right(values['left'], values['right']):
                position = 1

            words_indices = self.data[(self.data['line'] == line) & (self.data['column'] == column)]
            self.data.loc[words_indices.index, 'col_position'] = position

    def __column_is_centered(self, min_x, max_x):
        center_rate = (self.writable_center - min_x) / (max_x - self.writable_center)
        return min_x < self.writable_center < max_x and abs(1.0 - center_rate) < 0.1

    def __column_is_aligned_left(self, min_x, max_x):
        col_center = min_x + (max_x - min_x) * 0.5

        return (col_center < self.writable_center + self.writable_width * 0.1 and
                not min_x > self.writable_center)

    def __column_is_aligned_right(self, min_x, max_x):
        col_center = min_x + (max_x - min_x) * 0.5

        return col_center > self.writable_center

    def __assign_group_number(self):
        self.data['group'] = pd.Series(dtype='int')
        tolerance = self.writable_width * 0.005

        state = GroupState(
            line_cols=None,
            group_cols=None,
            prev_num_cols=None,
            centered={'prev': None, 'curr': None},
            prev_new_group=False,
            tolerance=tolerance,
            group_num=0
        )

        for _, line_words in self.data.groupby('line'):
            state.line_cols = self.__extract_line_columns(line_words)
            state.centered['curr'] = False
            for _, col in state.line_cols.items():
                if self.__column_is_centered(col['minX'], col['maxX']):
                    state.centered['curr'] = True

            new_group = self.__is_new_group(state)

            if new_group:
                state.group_num += 1

            self.data.loc[line_words.index, 'group'] = state.group_num

            # Update tracking variables
            state.prev_num_cols = len(state.line_cols)
            state.centered['prev'] = state.centered['curr']
            state.prev_new_group = new_group

            if state.group_cols is None or new_group:
                state.group_cols = state.line_cols
            else:
                self.__expand_group_columns(state.group_cols, state.line_cols)

    def __extract_line_columns(self, line_words):
        """Return a dict mapping column to position, minX, maxX."""
        columns_info = {}
        grouped = line_words.groupby('column').agg({
            'col_position': 'max',
            'left': 'min',
            'right': 'max'
        })

        for column, values in grouped.iterrows():
            columns_info[column] = {
                'position_max': values['col_position'],
                'minX': values['left'],
                'maxX': values['right'],
            }

        return columns_info

    def __is_new_group(self, state:GroupState):
        if (state.group_cols is None or
            state.prev_num_cols is None or
            state.centered['prev'] is None):
            return False

        if len(state.group_cols) != len(state.line_cols):
            if state.centered['curr'] or state.centered['prev']:
                return True
        elif (state.centered['curr'] != state.centered['prev'] and
              state.line_cols.get(0).get('position_max') == 0):
            return True

        for g_col in state.group_cols:
            g_position_max = state.group_cols[g_col]['position_max']
            for l_col in state.line_cols:
                if state.line_cols[l_col]['position_max'] != g_position_max:
                    continue

                if (not self.__columns_wrap_each_other(
                        state.line_cols[l_col], state.group_cols[g_col], state.tolerance) and
                    not state.prev_new_group):
                    return True
        return False

    def __columns_wrap_each_other(self, col1, col2, tolerance):
        return (
            (col1['minX'] > col2['minX'] - tolerance and col1['maxX'] < col2['maxX'] + tolerance) or
            (col2['minX'] > col1['minX'] - tolerance and col2['maxX'] < col1['maxX'] + tolerance)
        )

    def __expand_group_columns(self, group_cols, line_cols):
        for g_col in group_cols:
            position_max = group_cols[g_col]['position_max']
            for l_col in line_cols:
                if line_cols[l_col]['position_max'] != position_max:
                    continue

                group_cols[g_col]['minX'] = min(group_cols[g_col]['minX'], line_cols[l_col]['minX'])
                group_cols[g_col]['maxX'] = max(group_cols[g_col]['maxX'], line_cols[l_col]['maxX'])

    def __get_writable_boundaries(self):
        blocks = self.data[self.data['level'] == 2]
        min_x = blocks['left'].min()
        max_x = (blocks['left'] + blocks['width']).max()

        min_y = blocks['top'].min()
        max_y = (blocks['top'] + blocks['height']).max()

        return min_x, min_y, max_x, max_y

class OcrPage():
    """This class stores the information contained in a single page
    of a document and provides functions to process it.
    """

    def __init__(self, image: Image, cache_file:str|None=None):
        self.image = image
        self.cache_file = cache_file

        self.boundaries = {
            'left': 0.05,
            'top': 0.1,
            'right': 0.95,
            'bottom': 0.95,
        }

        self.data = self.__get_data_from_image()
        self.width = self.data.loc[0, 'width']
        self.height = self.data.loc[0, 'height']
        self.__normalize_data()

        reconstructor = DataReconstructor(self.data)
        self.data = reconstructor.get_reconstructed()

    def get_text(self, remove_headers: bool=False) -> str:
        """Return the reconstructed text of the page (without Tesseract errors)."""
        if remove_headers:
            data = self.data[
                (self.data['left'] > self.boundaries['left']) &
                (self.data['top'] > self.boundaries['top']) &
                (self.data['right'] < self.boundaries['right']) &
                (self.data['bottom'] < self.boundaries['bottom'])]
        else:
            data = self.data
        data = data.sort_values(['line', 'left'])
        texts_by_line = data.groupby(['group', 'col_position', 'line'])['text'].apply(' '.join)
        texts_by_column = texts_by_line.groupby(['group', 'col_position']).apply('\n'.join)
        texts_by_group = texts_by_column.groupby('group').apply('\n'.join)

        return '\n'.join(texts_by_group)

    def get_indices(self) -> list[int]:
        """Return the indices of the reconstructed data."""
        data = self.data.sort_values(['line', 'left']).reset_index()
        indices_by_line = data.groupby(['group', 'col_position', 'line'])['index'].agg(list)
        indices_by_colum = indices_by_line.groupby(['group', 'col_position']).sum()
        indices_by_group = indices_by_colum.groupby('group').sum()

        return list(indices_by_group.sum())

    def get_raw_text(self) -> str:
        """Return the text as Tesseract returns it."""
        data = self.data.dropna()
        texts_by_line = data.groupby(['block_num', 'par_num', 'line_num'])['text'].apply(' '.join)
        texts_by_paragraph = texts_by_line.groupby(['block_num', 'par_num']).apply('\n'.join)
        texts_by_block = texts_by_paragraph.groupby('block_num').apply('\n'.join)

        return '\n'.join(texts_by_block)

    def get_words(self, suffix = '') -> pd.DataFrame:
        """Get the reconstructed text and split it into words into a dataframe."""
        text = self.get_text()
        if text == '':
            return pd.DataFrame(columns=['word'])

        indices = self.get_indices()
        words = [(idx, f'{w}{suffix}') for idx, w in zip(indices, text.split())]
        df_words = pd.DataFrame(words, columns=['ocr_idx', 'word'])
        df_words.set_index('ocr_idx', inplace=True)

        return df_words

    def get_raw_words(self, suffix = '') -> list[str]:
        """Get the text as Tesseract returns it and split it into words into a dataframe."""
        return [f'{w}{suffix}' for w in self.data.dropna()['text']]

    def show_detection(self, level=2):
        """Show an image of the detected regions by Tesseract."""
        canvas = self.image.copy()
        draw = ImageDraw.Draw(canvas)

        # Draw regions
        for _, row in self.data.iterrows():
            if row['level'] == level:
                (x, y, w, h) = (int(row['left']*self.width),
                                int(row['top']*self.height),
                                int(row['width']*self.width),
                                int(row['height']*self.height))
                draw.rectangle(((x, y), (x + w, y + h)), outline="green", width=10)

        plt.imshow(canvas)
        plt.show()

    def show_relevant_detection(self):
        """Show an image of the reconstructed regions after removing errors from Tesseract."""
        canvas = self.image.copy()
        draw = ImageDraw.Draw(canvas)

        # Draw regions
        grouped = self.data.dropna().groupby(['group']).agg({
            'left': 'min',
            'top': 'min',
            'bottom': 'max',
            'right': 'max'})
        for _, values in grouped.iterrows():
            (x_1, y_1, x_2, y_2) = (int(values['left']*self.width),
                                    int(values['top']*self.height),
                                    int(values['right']*self.width),
                                    int(values['bottom']*self.height))
            draw.rectangle(((x_1, y_1), (x_2, y_2)), outline="green", width=10)

        plt.imshow(canvas)
        plt.show()

    def get_data(self) -> pd.DataFrame:
        """Get the full DataFrame of OCR info."""
        return self.data

    def __get_data_from_image(self):
        if self.cache_file is not None and os.path.exists(self.cache_file):
            data = pd.read_csv(self.cache_file, sep=',')
        else:
            data = pytesseract.image_to_data(self.image,
                                             lang='spa',
                                             output_type=pytesseract.Output.DATAFRAME)
            data.to_csv(self.cache_file, sep=',', index=False)

        # Corregimos condición que detecta 'nan' en texto como NaN en float
        if len(data[data['text'].isna() & (data['conf'] != -1)]) > 0:
            data.loc[data['text'].isna() & (data['conf'] != -1), 'text'] = '#nan#'

        return data

    def __normalize_data(self):
        self.data['left'] = self.data['left'] / self.data.loc[0, 'width']
        self.data['width'] = self.data['width'] / self.data.loc[0, 'width']
        self.data['top'] = self.data['top'] / self.data.loc[0, 'height']
        self.data['height'] = self.data['height'] / self.data.loc[0, 'height']

class OcrPdfParser():
    """This class uses Google Tesseract to parse the content of PDF
    files into texto.

    :param pdf_path: Path to the PDF file to parse
    :type pdf_path: str
    :param cache_dir: Path to the directory to be used as cache.
    :type cache_dir: str
    :param keep_cache: True to keep the cache of images and Tessearct data. Defaults to False.
    :type keep_cache: bool
    """

    def __init__(self, pdf_path: str, cache_dir: str = './.cache', keep_cache:bool = False):
        self.pdf_path = pdf_path
        self.cache_dir = cache_dir
        self.keep_cache = keep_cache

        with open(self.pdf_path, 'rb') as f:
            file_md5 = hashlib.md5(f.read()).hexdigest()
        self.cache_subfolder = os.path.join(self.cache_dir, file_md5)

        if not self.__cache_is_valid():
            self.__create_cache()

    def __del__(self):
        if not self.keep_cache:
            self.clear_cache()

    def get_text(self, page_separator: str = '\n', remove_headers:bool = False) -> str:
        """Return the text of all the pages in the document."""
        text = ""
        for page in self.get_pages():
            text += page.get_text(remove_headers) + page_separator

        return text

    def get_pages(self) -> Iterator[OcrPage]:
        """Read each page of the document and return its information as an Iterator.

        :return:
        :rtype: Iterator[str]
        """
        pages_path = glob.glob(f'{self.cache_subfolder}/0001-*.jpg')
        for page_path in sorted(pages_path):
            basepath = os.path.splitext(page_path)[0]
            yield OcrPage(Image.open(page_path), cache_file=f'{basepath}.csv')

    def get_page(self, page_num: int) -> OcrPage:
        """Return a spific page of the document."""
        basepath = f'{self.cache_subfolder}/0001-{page_num+1:02d}'
        return OcrPage(Image.open(f'{basepath}.jpg'), f'{basepath}.csv')

    def clear_cache(self):
        """Delete the cache sub-directory for this file. If main directory is empty then
        it is deleted aswell."""
        tmp_cache = f'{self.cache_subfolder}.tmp'

        if os.path.exists(self.cache_subfolder):
            shutil.rmtree(self.cache_subfolder)
        if os.path.exists(tmp_cache):
            shutil.rmtree(tmp_cache)

        if not os.listdir(self.cache_dir):
            os.rmdir(self.cache_dir)

    def __cache_is_valid(self) -> bool:
        return os.path.exists(self.cache_subfolder)

    def __create_cache(self):
        tmp_cache = f'{self.cache_subfolder}.tmp'

        if os.path.exists(self.cache_subfolder):
            shutil.rmtree(self.cache_subfolder)
        if os.path.exists(tmp_cache):
            shutil.rmtree(tmp_cache)

        os.makedirs(self.cache_subfolder, exist_ok=True)
        os.makedirs(tmp_cache, exist_ok=True)

        _ = pdf2image.convert_from_path(self.pdf_path,
                                        output_folder=tmp_cache,
                                        fmt='jpeg',
                                        dpi=1000,
                                        output_file='')
        os.rename(tmp_cache, self.cache_subfolder) # Just to make shure all information is there
